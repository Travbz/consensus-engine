.
.
./.gitignore
./LICENSE
./README.md
./consensus_engine.db
./dir.sh
./pyproject.toml
./queries
./query.txt
./src
./src/consensus_engine
./src/consensus_engine/cli.py
./src/consensus_engine/config
./src/consensus_engine/config/__init__.py
./src/consensus_engine/config/settings.py
./src/consensus_engine/database
./src/consensus_engine/database/models.py
./src/consensus_engine/deliberation
./src/consensus_engine/deliberation/__init__.py
./src/consensus_engine/deliberation/consensus.py
./src/consensus_engine/engine.py
./src/consensus_engine/init.py
./src/consensus_engine/models
./src/consensus_engine/models/__init__.py
./src/consensus_engine/models/anthropic.py
./src/consensus_engine/models/base.py
./src/consensus_engine/models/openai.py
./src/consensus_engine/web.py
./tests
./tests/__init__.py
./tests/test_engine.py
./tests/test_models.py

=== Tree View ===

/Users/travops/consensus-engine
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ consensus_engine.db
‚îú‚îÄ‚îÄ dir.sh
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ queries
‚îú‚îÄ‚îÄ query.txt
‚îú‚îÄ‚îÄ src
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ consensus_engine
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ cli.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ config
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ settings.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ database
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ models.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ deliberation
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ consensus.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ engine.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ init.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ models
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ anthropic.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ base.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ openai.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ web.py
‚îî‚îÄ‚îÄ tests
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_engine.py
    ‚îî‚îÄ‚îÄ test_models.py

9 directories, 22 files

=== File Contents ===


#LICENSE
MIT License

Copyright (c) 2024 TravBz

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

#consensus_engine.db (binary file - contents excluded)

#pyproject.toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "consensus-engine"
version = "0.1.0"
authors = [
    { name = "Your Name", email = "your.email@example.com" },
]
description = "A consensus engine for orchestrating discussions between multiple LLMs"
readme = "README.md"
requires-python = ">=3.8"
dependencies = [
    "click>=8.0.0",
    "openai>=1.0.0",
    "anthropic>=0.5.0",
    "asyncio>=3.4.3",
    "sqlalchemy>=2.0.0",
    "aiohttp>=3.8.0",
    "python-dotenv>=0.19.0",
    "nltk>=3.8.1",
    "scikit-learn>=1.0.2",
    "numpy>=1.21.0",
    "gradio>=4.0.0",
    "rich>=13.0.0"
]

[project.scripts]
consensus-engine = "consensus_engine.cli:cli"
consensus-web = "consensus_engine.web:main"

[tool.hatch.build.targets.wheel]
packages = ["src/consensus_engine"]
#tests/test_engine.py
"""coming soon"""
#tests/__init__.py (binary file - contents excluded)

#tests/test_models.py
"""coming soon"""
#README.md
# Consensus Engine

A tool for orchestrating discussions between multiple LLMs to reach consensus.

## Features

	‚Ä¢	Multi-LLM consensus-building framework.
	‚Ä¢	Supports OpenAI and Anthropic LLMs.
	‚Ä¢	CLI and Gradio-based web interface.
	‚Ä¢	Persistent storage for discussions (SQLite by default).
	‚Ä¢	Modular design for extensibility.

## Prerequisites

	‚Ä¢	Python 3.8 or later
	‚Ä¢	Git
	‚Ä¢	API keys for OpenAI and Anthropic models
	‚Ä¢	SQLite (bundled with Python standard library)

## Installation



```bash
git clone git@github.com:Travbz/consensus-engine.git

cd consensus-engine

python3 -m venv venv

source venv/bin/activate

pip install -e .
```

## Usage
* navigate to consensus-engine/src/consensus_engine/config/settings.py and set your API keys
* Update system prompts to your liking
* Modify token usage limits to your liking
* Modify deliberation parameters to your liking


### Command Line Interface
```bash
# Set your API keys
export OPENAI_API_KEY="your-key-here"
export ANTHROPIC_API_KEY="your-key-here"

# Run the CLI
consensus-engine discuss
```

### Web Interface (Gradio)
```bash
# Run the web interface
consensus-web

# Or with custom port
consensus-web --port 8080

# Or with specific host
consensus-web --host 0.0.0.0 --port 8080

# For CLI with debug output
consensus-engine discuss --debug

# For web interface
consensus-engine web --port 8080 --debug

# To view a past discussion
consensus-engine view-discussion 1 --debug

```

Then open http://localhost:7860 (or your specified port) in your browser.
#.gitignore
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/


*.db
#dir.sh
#!/bin/bash

# Check if directory path is provided
if [ $# -ne 1 ]; then
    echo "Usage: $0 <directory_path>"
    exit 1
fi

# Get directory path and name
DIR_PATH=$(realpath "$1")
DIR_NAME=$(basename "$DIR_PATH")
DEVELOPMENT_DIR="$HOME/consensus"
QUERIES_DIR="$DEVELOPMENT_DIR/queries"
OUTPUT_FILE="query.txt"

# Check if directory exists
if [ ! -d "$DIR_PATH" ]; then
    echo "Error: Directory does not exist"
    exit 1
fi

# Check if Development/queries directory exists, if not create it
if [ ! -d "$QUERIES_DIR" ]; then
    mkdir -p "$QUERIES_DIR"
    if [ $? -ne 0 ]; then
        echo "Error: Could not create queries directory"
        exit 1
    fi
fi

# Check if tree command is available
if ! command -v tree &> /dev/null; then
    echo "Error: 'tree' command not found. Please install it first."
    exit 1
fi

# Create directory structure with find
{
    echo "$DIR_PATH"
    find "$DIR_PATH" \
        -name ".git" -prune -o \
        -name ".terraform" -prune -o \
        -name "node_modules" -prune -o \
        -name "venv" -prune -o \
        -name "__pycache__" -prune -o \
        -print | sort
} | sed -e "s;$DIR_PATH;.;g" | grep -v ".terraform/" > "$OUTPUT_FILE"

# Add a separator and tree view
echo -e "\n=== Tree View ===\n" >> "$OUTPUT_FILE"

# Generate tree view excluding unwanted directories
tree -I '.git|.terraform*|node_modules|venv|__pycache__|*.tfstate*|*.tfvars*|override.tf|override.tf.json|*_override.tf|*_override.tf.json|.terraformrc|terraform.rc|.terragrunt*|*.lock.json|*.lock|.env*|.DS_Store|*.log' "$DIR_PATH" >> "$OUTPUT_FILE"

# Add a separator for file contents
echo -e "\n=== File Contents ===\n" >> "$OUTPUT_FILE"

# Find all files recursively, with proper exclusions
find "$DIR_PATH" -type f \
    -not -path "*/\\.git/*" \
    -not -path "*/\\.terraform*/*" \
    -not -path "*/node_modules/*" \
    -not -path "*/venv/*" \
    -not -path "*/__pycache__/*" \
    -not -name "*.tfstate*" \
    -not -name "*.tfvars*" \
    -not -name "override.tf" \
    -not -name "override.tf.json" \
    -not -name "*_override.tf" \
    -not -name "*_override.tf.json" \
    -not -name ".terraformrc" \
    -not -name "terraform.rc" \
    -not -path "*/\\.terragrunt*/*" \
    -not -name "*.lock.json" \
    -not -name "*.lock" \
    -not -name ".env*" \
    -not -name ".DS_Store" \
    -not -name "*.log" \
    -not -name "$(basename "$OUTPUT_FILE")" \
    -print0 | \
    while IFS= read -r -d "" file; do
        # Get relative path from the directory
        rel_path="${file#$DIR_PATH/}"
        
        # Check if the file is binary
        if file "$file" | grep -q "text"; then
            # Add file header and contents to output file
            echo -e "\n#$rel_path" >> "$OUTPUT_FILE"
            cat "$file" >> "$OUTPUT_FILE"
        else
            # For binary files, just note that it's binary
            echo -e "\n#$rel_path (binary file - contents excluded)" >> "$OUTPUT_FILE"
        fi
    done

echo "Directory snapshot has been saved to: $OUTPUT_FILE"
#src/consensus_engine/deliberation/__init__.py (binary file - contents excluded)

#src/consensus_engine/deliberation/consensus.py
"""Plans to break the engine into seperate classes, coming soon"""
#src/consensus_engine/database/models.py
"""Database models for the Consensus Engine."""
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
import datetime

Base = declarative_base()

class Discussion(Base):
    __tablename__ = 'discussions'
    
    id = Column(Integer, primary_key=True)
    prompt = Column(Text, nullable=False)
    started_at = Column(DateTime, default=datetime.datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)
    consensus_reached = Column(Integer, default=0)
    final_consensus = Column(Text, nullable=True)
    
    rounds = relationship("DiscussionRound", back_populates="discussion")

class DiscussionRound(Base):
    __tablename__ = 'discussion_rounds'
    
    id = Column(Integer, primary_key=True)
    discussion_id = Column(Integer, ForeignKey('discussions.id'))
    round_number = Column(Integer, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow)
    
    discussion = relationship("Discussion", back_populates="rounds")
    responses = relationship("LLMResponse", back_populates="round")

class LLMResponse(Base):
    __tablename__ = 'llm_responses'
    
    id = Column(Integer, primary_key=True)
    round_id = Column(Integer, ForeignKey('discussion_rounds.id'))
    llm_name = Column(String(100), nullable=False)
    response_text = Column(Text, nullable=False)
    confidence_score = Column(Float, nullable=True)
    created_at = Column(DateTime, default=datetime.datetime.utcnow)
    
    round = relationship("DiscussionRound", back_populates="responses")
#src/consensus_engine/web.py
"""Web interface for the Consensus Engine using Gradio."""
import gradio as gr
import asyncio
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from .engine import ConsensusEngine
from .models.openai import OpenAILLM
from .models.anthropic import AnthropicLLM
from .database.models import Base, Discussion
import logging

logger = logging.getLogger(__name__)

def get_db_session():
    database_url = os.getenv("CONSENSUS_ENGINE_DB_URL", "sqlite:///consensus_engine.db")
    engine = create_engine(database_url)
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    return Session()

class GradioInterface:
    def __init__(self):
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        self.llms = [
            OpenAILLM(self.openai_key),
            AnthropicLLM(self.anthropic_key)
        ]
        self.db_session = get_db_session()

    def _get_past_discussions(self):
        """Retrieve past discussions from the database."""
        discussions = self.db_session.query(Discussion).all()
        return [
            {
                "id": discussion.id,
                "prompt": discussion.prompt,
                "timestamp": discussion.started_at.strftime("%Y-%m-%d %H:%M:%S"),
                "consensus_reached": bool(discussion.consensus_reached),
                "final_consensus": discussion.final_consensus
            }
            for discussion in discussions
        ]

    def _get_discussion_details(self, discussion_id):
        """Retrieve the details of a specific discussion."""
        discussion = self.db_session.query(Discussion).get(discussion_id)
        if not discussion:
            return None

        rounds = [
            {
                "round_number": round.round_number,
                "responses": [
                    {
                        "llm_name": response.llm_name,
                        "response_text": response.response_text
                    }
                    for response in round.responses
                ]
            }
            for round in discussion.rounds
        ]

        return {
            "id": discussion.id,
            "prompt": discussion.prompt,
            "started_at": discussion.started_at.strftime("%Y-%m-%d %H:%M:%S"),
            "completed_at": (
                discussion.completed_at.strftime("%Y-%m-%d %H:%M:%S")
                if discussion.completed_at
                else None
            ),
            "consensus_reached": bool(discussion.consensus_reached),
            "final_consensus": discussion.final_consensus,
            "rounds": rounds
        }

    def launch(self, host=None, port=None, debug=False):
        with gr.Blocks() as interface:
            with gr.Row():
                gr.Markdown("### LLM Consensus Engine: View Past Discussions")

            with gr.Row():
                with gr.Column():
                    discussions = gr.Dropdown(
                        label="Select a Discussion",
                        choices=[],
                        interactive=True,
                    )
                with gr.Column():
                    fetch_btn = gr.Button("Fetch Details")
            
            with gr.Row():
                discussion_details = gr.Textbox(
                    label="Discussion Details",
                    lines=20,
                    interactive=False,
                    container=True
                )

            # Fetch discussions on app launch
            def populate_discussions():
                past_discussions = self._get_past_discussions()
                options = [f"{d['id']}: {d['prompt'][:50]} ({d['timestamp']})" for d in past_discussions]
                return options, past_discussions

            def fetch_discussion_details(selected_id, discussions):
                discussion_id = int(selected_id.split(":")[0])
                details = self._get_discussion_details(discussion_id)
                if not details:
                    return "No details found for the selected discussion."
                formatted_details = (
                    f"ID: {details['id']}\n"
                    f"Prompt: {details['prompt']}\n"
                    f"Started At: {details['started_at']}\n"
                    f"Completed At: {details['completed_at'] or 'N/A'}\n"
                    f"Consensus Reached: {'Yes' if details['consensus_reached'] else 'No'}\n"
                    f"Final Consensus:\n{details['final_consensus'] or 'N/A'}\n\n"
                    "Rounds:\n"
                )
                for round in details["rounds"]:
                    formatted_details += f"  Round {round['round_number']}:\n"
                    for response in round["responses"]:
                        formatted_details += (
                            f"    {response['llm_name']}:\n"
                            f"      {response['response_text']}\n"
                        )
                return formatted_details

            discussions.change(
                fn=fetch_discussion_details,
                inputs=[discussions],
                outputs=[discussion_details]
            )

            fetch_btn.click(
                fn=populate_discussions,
                inputs=[],
                outputs=[discussions]
            )

            interface.launch(
                server_port=port if port else 7860,
                share=False,
                inbrowser=True,
                server_name=host if host else "127.0.0.1",
                debug=debug
            )

def main():
    app = GradioInterface()
    app.launch()

if __name__ == "__main__":
    main()
#src/consensus_engine/config/__init__.py (binary file - contents excluded)

#src/consensus_engine/config/settings.py
"""Configuration settings for the Consensus Engine."""
import os
import logging

# Model Settings
OPENAI_CONFIG = {
    "model": "gpt-4-turbo-preview",
    "temperature": 0.7,
    "max_tokens": 2000,
    "system_prompt": "You are a cooperative AI participating in a multi-AI consensus discussion. Your goal is to collaboratively identify common ground and efficiently produce a clear, concise, and actionable response to the original query."
}

ANTHROPIC_CONFIG = {
    "model": "claude-3-sonnet-20240229",
    "temperature": 0.7,
    "max_tokens": 2000,
    "system_prompt": "You are a cooperative AI participating in a multi-AI consensus discussion. Your goal is to collaboratively identify common ground and efficiently produce a clear, concise, and actionable response to the original query."

}

# Consensus Settings
MAX_ITERATIONS = 4
CONSENSUS_THRESHOLD = 0.75

# System Prompts for Different Stages
DELIBERATION_PROMPT = """You are tasked with analyzing the provided responses to identify common ground and propose a unified solution that satisfies the original query. 

- Focus on the core objective of the query and provide a concise, actionable, and user-centric response.
- Avoid philosophical debates or excessive elaboration; prioritize clarity and simplicity.
- Highlight the strengths of each response, resolve any conflicting points, and synthesize a response that directly addresses the user's needs.
- Ensure the final response is easy to understand, aligns with the original query, and offers a practical solution."""

# Database Settings
DATABASE_URL = os.getenv("CONSENSUS_ENGINE_DB_URL", "sqlite:///consensus_engine.db")

# Logging Settings
LOG_LEVEL = os.getenv("CONSENSUS_ENGINE_LOG_LEVEL", "INFO")
LOG_FORMAT = '%(message)s'
DETAILED_LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
LOG_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'

# Convert string log level to logging constant
LOG_LEVEL_MAP = {
    "DEBUG": logging.DEBUG,
    "INFO": logging.INFO,
    "WARNING": logging.WARNING,
    "ERROR": logging.ERROR,
    "CRITICAL": logging.CRITICAL
}

LOG_LEVEL_NUM = LOG_LEVEL_MAP.get(LOG_LEVEL.upper(), logging.INFO)
#src/consensus_engine/models/__init__.py (binary file - contents excluded)

#src/consensus_engine/models/openai.py
"""OpenAI model implementation."""
from typing import Dict, Optional
from openai import AsyncOpenAI
import logging
from .base import BaseLLM
from ..config.settings import OPENAI_CONFIG, DELIBERATION_PROMPT

logger = logging.getLogger(__name__)

class OpenAILLM(BaseLLM):
    def __init__(self, api_key: str, model: str = None):
        super().__init__(api_key, model or OPENAI_CONFIG["model"])
        self.client = AsyncOpenAI(api_key=api_key)
    
    async def generate_response(self, prompt: str) -> str:
        logger.info(f"Getting response from OpenAI ({self.model})")
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "system",
                "content": OPENAI_CONFIG["system_prompt"]
            }, {
                "role": "user",
                "content": prompt
            }],
            temperature=OPENAI_CONFIG["temperature"],
            max_tokens=OPENAI_CONFIG["max_tokens"]
        )
        return response.choices[0].message.content
    
    async def deliberate(self, prompt: str, responses: Dict[str, str]) -> str:
        logger.info("OpenAI deliberating on responses")
        deliberation_prompt = (
            f"Original prompt: {prompt}\n\n"
            "Responses:\n"
        )
        
        for llm_name, response in responses.items():
            deliberation_prompt += f"\n{llm_name}: {response}\n"
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "system",
                "content": DELIBERATION_PROMPT
            }, {
                "role": "user",
                "content": deliberation_prompt
            }],
            temperature=OPENAI_CONFIG["temperature"],
            max_tokens=OPENAI_CONFIG["max_tokens"]
        )
        return response.choices[0].message.content
    
    @property
    def name(self) -> str:
        return "OpenAI"
#src/consensus_engine/models/anthropic.py
"""Anthropic model implementation."""
from typing import Dict, Optional
from anthropic import AsyncAnthropic
import logging
from .base import BaseLLM
from ..config.settings import ANTHROPIC_CONFIG, DELIBERATION_PROMPT

logger = logging.getLogger(__name__)

class AnthropicLLM(BaseLLM):
    def __init__(self, api_key: str, model: str = None):
        super().__init__(api_key, model or ANTHROPIC_CONFIG["model"])
        self.client = AsyncAnthropic(api_key=api_key)
    
    async def generate_response(self, prompt: str) -> str:
        logger.info(f"Getting response from Anthropic ({self.model})")
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=ANTHROPIC_CONFIG["max_tokens"],
            messages=[{
                "role": "user",
                "content": prompt
            }],
            system=ANTHROPIC_CONFIG["system_prompt"],
            temperature=ANTHROPIC_CONFIG["temperature"]
        )
        return response.content[0].text
    
    async def deliberate(self, prompt: str, responses: Dict[str, str]) -> str:
        logger.info("Anthropic deliberating on responses")
        deliberation_prompt = (
            f"Original prompt: {prompt}\n\n"
            "Responses:\n"
        )
        
        for llm_name, response in responses.items():
            deliberation_prompt += f"\n{llm_name}: {response}\n"
        
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=ANTHROPIC_CONFIG["max_tokens"],
            messages=[{
                "role": "user",
                "content": deliberation_prompt
            }],
            system=DELIBERATION_PROMPT,
            temperature=ANTHROPIC_CONFIG["temperature"]
        )
        return response.content[0].text
    
    @property
    def name(self) -> str:
        return "Anthropic"
#src/consensus_engine/models/base.py
"""Base class for LLM implementations."""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

class BaseLLM(ABC):
    """Base class for LLM implementations."""
    
    def __init__(self, api_key: str, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model
    
    @abstractmethod
    async def generate_response(self, prompt: str) -> str:
        """Generate a response for the given prompt."""
        pass
    
    @abstractmethod
    async def deliberate(self, prompt: str, responses: Dict[str, str]) -> str:
        """Deliberate on the responses from other LLMs and provide feedback."""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Return the name of the LLM provider."""
        pass
#src/consensus_engine/engine.py
"""Core consensus engine implementation."""
from typing import List, Dict, Optional, Callable, Awaitable
import asyncio
from sqlalchemy.orm import Session
from .models.base import BaseLLM
from .database.models import Discussion, DiscussionRound, LLMResponse
from datetime import datetime
import logging
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from .config.settings import MAX_ITERATIONS, CONSENSUS_THRESHOLD

logger = logging.getLogger(__name__)

class ConsensusEngine:
    def __init__(self, llms: List[BaseLLM], db_session: Session):
        self.llms = llms
        self.db = db_session
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
        except:
            logger.warning("NLTK data download failed, falling back to simpler consensus check")

    def _calculate_similarity(self, responses: Dict[str, str]) -> float:
        """Calculate semantic similarity between responses using TF-IDF and cosine similarity."""
        try:
            texts = list(responses.values())
            vectorizer = TfidfVectorizer(stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(texts)
            similarities = cosine_similarity(tfidf_matrix)
            avg_similarity = (similarities.sum() - len(texts)) / (len(texts) * (len(texts) - 1))
            logger.debug(f"Average similarity score: {avg_similarity:.3f}")
            return avg_similarity
        except Exception as e:
            logger.warning(f"Error calculating similarity: {e}")
            return 0.0

    def _extract_key_points(self, response: str) -> str:
        """Extract main points from a response."""
        try:
            sentences = sent_tokenize(response)[:3]  # Get first three sentences
            return ' '.join(sentences)
        except Exception as e:
            logger.warning(f"Error extracting key points: {e}")
            return response[:200] + "..."  # Fallback to simple truncation

    def _identify_key_differences(self, responses: Dict[str, str]) -> str:
        """Identify main points of disagreement between responses."""
        try:
            differences = []
            for name1, response1 in responses.items():
                for name2, response2 in responses.items():
                    if name1 < name2:  # Avoid comparing same pairs twice
                        similarity = self._calculate_pairwise_similarity(response1, response2)
                        if similarity < CONSENSUS_THRESHOLD:
                            key_points1 = self._extract_key_points(response1)
                            key_points2 = self._extract_key_points(response2)
                            differences.append(f"{name1} vs {name2} (similarity: {similarity:.2f}):")
                            differences.append(f"- {name1}: {key_points1}")
                            differences.append(f"- {name2}: {key_points2}\n")
            
            return "\n".join(differences) if differences else "Models are fairly aligned but haven't reached consensus threshold."
        except Exception as e:
            logger.warning(f"Error identifying differences: {e}")
            return "Could not analyze differences in detail."

    def _calculate_pairwise_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity between two texts."""
        try:
            vectorizer = TfidfVectorizer(stop_words='english')
            tfidf_matrix = vectorizer.fit_transform([text1, text2])
            similarity = cosine_similarity(tfidf_matrix)[0][1]
            return similarity
        except Exception as e:
            logger.warning(f"Error calculating pairwise similarity: {e}")
            return 0.0

    def _check_consensus(self, responses: Dict[str, str]) -> bool:
        """Check if consensus has been reached."""
        if len(responses) <= 1:
            return True
        
        similarity_score = self._calculate_similarity(responses)
        has_consensus = similarity_score >= CONSENSUS_THRESHOLD
        logger.info(f"Consensus check: similarity={similarity_score:.3f}, threshold={CONSENSUS_THRESHOLD}, reached={has_consensus}")
        return has_consensus

    async def _create_unified_response(self, responses: Dict[str, str]) -> str:
        """Create a unified response when consensus is reached."""
        try:
            synthesis_prompt = (
                "The following responses have reached consensus. "
                "Please create a unified response that captures all key points and shared understanding "
                "in a clear and concise way:\n\n"
            )
            
            for llm_name, response in responses.items():
                synthesis_prompt += f"{llm_name}:\n{response}\n\n"
            
            logger.info("Creating unified consensus response...")
            unified_response = await self.llms[0].generate_response(synthesis_prompt)
            return unified_response
            
        except Exception as e:
            logger.warning(f"Error creating unified response: {e}")
            # Fallback: Concatenate responses with headers
            unified = "Consensus Reached - Combined Responses:\n\n"
            for llm_name, response in responses.items():
                unified += f"From {llm_name}:\n{response}\n\n"
            return unified

    async def discuss(self, prompt: str, progress_callback: Optional[Callable[[str], Awaitable[None]]] = None) -> Dict[str, str]:
        """
        Conduct a consensus discussion.
        
        Args:
            prompt: The initial prompt/question
            progress_callback: Optional callback for progress updates
        """
        async def update_progress(msg: str):
            if progress_callback:
                await progress_callback(msg)
            logger.info(msg)

        # Create new discussion
        discussion = Discussion(prompt=prompt)
        self.db.add(discussion)
        self.db.commit()
        
        try:
            await update_progress("üöÄ Starting consensus discussion...")
            await update_progress(f"üìù Prompt: {prompt}\n")
            
            # Get initial responses
            responses = {}
            current_round = DiscussionRound(
                discussion_id=discussion.id,
                round_number=0
            )
            self.db.add(current_round)
            self.db.commit()
            
            # Initial responses
            for llm in self.llms:
                await update_progress(f"üí≠ Getting response from {llm.name}...")
                response = await llm.generate_response(prompt)
                responses[llm.name] = response
                llm_response = LLMResponse(
                    round_id=current_round.id,
                    llm_name=llm.name,
                    response_text=response
                )
                self.db.add(llm_response)
                key_points = self._extract_key_points(response)
                await update_progress(f"\n{llm.name} main points:\n{key_points}\n")
            
            self.db.commit()
            
            # Deliberation rounds
            iteration = 0
            while iteration < MAX_ITERATIONS:
                similarity = self._calculate_similarity(responses)
                await update_progress(f"\nüìä Current agreement level: {similarity:.2f}")
                
                if self._check_consensus(responses):
                    await update_progress("\nüéâ Consensus reached!")
                    unified_response = await self._create_unified_response(responses)
                    
                    discussion.consensus_reached = 1
                    discussion.final_consensus = unified_response
                    discussion.completed_at = datetime.utcnow()
                    self.db.commit()
                    
                    await update_progress("\nüìù Unified Consensus Response:")
                    await update_progress(unified_response)
                    
                    return {
                        "consensus": unified_response,
                        "individual_responses": responses
                    }
                
                iteration += 1
                await update_progress(f"\nü§î Round {iteration}/{MAX_ITERATIONS}: Models discussing differences...")
                
                current_round = DiscussionRound(
                    discussion_id=discussion.id,
                    round_number=iteration
                )
                self.db.add(current_round)
                self.db.commit()
                
                # Show key differences being discussed
                differences = self._identify_key_differences(responses)
                await update_progress(f"\nMain points of discussion:\n{differences}\n")
                
                # Get new responses
                new_responses = {}
                for llm in self.llms:
                    await update_progress(f"üí≠ Getting {llm.name}'s thoughts on the discussion...")
                    deliberation = await llm.deliberate(prompt, responses)
                    new_responses[llm.name] = deliberation
                    llm_response = LLMResponse(
                        round_id=current_round.id,
                        llm_name=llm.name,
                        response_text=deliberation
                    )
                    self.db.add(llm_response)
                    key_points = self._extract_key_points(deliberation)
                    await update_progress(f"\n{llm.name}'s main points:\n{key_points}\n")
                
                self.db.commit()
                responses = new_responses

            await update_progress("\n‚ö†Ô∏è Maximum rounds reached without consensus")
            discussion.completed_at = datetime.utcnow()
            self.db.commit()
            return responses
            
        except Exception as e:
            logger.error(f"Error during discussion: {str(e)}", exc_info=True)
            discussion.completed_at = datetime.utcnow()
            self.db.commit()
            raise e

    async def load_discussion(self, discussion_id: int) -> Optional[Dict[str, List[Dict[str, str]]]]:
        """Load a previous discussion from the database."""
        discussion = self.db.query(Discussion).get(discussion_id)
        if not discussion:
            return None
            
        result = {
            'prompt': discussion.prompt,
            'consensus_reached': bool(discussion.consensus_reached),
            'final_consensus': discussion.final_consensus,
            'rounds': []
        }
        
        for round in discussion.rounds:
            round_responses = {
                'round_number': round.round_number,
                'responses': []
            }
            for response in round.responses:
                round_responses['responses'].append({
                    'llm_name': response.llm_name,
                    'response': response.response_text
                })
            result['rounds'].append(round_responses)
            
        return result
#src/consensus_engine/cli.py
"""Command-line interface for the Consensus Engine."""
import asyncio
import click
import os
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.layout import Layout
from rich import print as rprint
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from .engine import ConsensusEngine
from .models.openai import OpenAILLM
from .models.anthropic import AnthropicLLM
from .database.models import Base
from .config.settings import LOG_LEVEL_NUM
import logging

# Configure logging
logging.basicConfig(level=LOG_LEVEL_NUM)
console = Console()

def get_db_session():
    """Initialize and return a database session."""
    database_url = os.getenv("CONSENSUS_ENGINE_DB_URL", "sqlite:///consensus_engine.db")
    engine = create_engine(database_url)
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    return Session()

async def update_display(msg: str):
    """Update the console display with new messages."""
    console.print(Panel(msg, border_style="blue"))

@click.group()
def cli():
    """Consensus Engine - Orchestrate discussions between multiple LLMs."""
    pass

@cli.command()
@click.option('--debug', is_flag=True, help='Enable debug logging')
def discuss(debug):
    """Start a consensus discussion with multiple LLMs."""
    if debug:
        logging.getLogger().setLevel(logging.DEBUG)

    # Check API keys
    openai_key = os.getenv("OPENAI_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    
    if not openai_key or not anthropic_key:
        console.print("[red]Error: Missing API keys[/red]")
        console.print("Please set the following environment variables:")
        console.print("  - OPENAI_API_KEY")
        console.print("  - ANTHROPIC_API_KEY")
        return

    # Initialize LLMs
    llms = [
        OpenAILLM(openai_key),
        AnthropicLLM(anthropic_key)
    ]
    
    db_session = get_db_session()
    engine = ConsensusEngine(llms, db_session)
    
    try:
        # Get prompt
        prompt = console.input("\n[bold green]Enter your prompt:[/bold green] ")
        if not prompt.strip():
            console.print("[red]Error: Prompt cannot be empty[/red]")
            return
        
        console.print("\n[bold blue]Starting consensus discussion...[/bold blue]\n")
        
        # Run discussion
        responses = asyncio.run(engine.discuss(prompt, update_display))
        
        # Display responses based on whether consensus was reached
        if isinstance(responses, dict) and "consensus" in responses:
            console.print("\n[bold green]üéâ Consensus Reached![/bold green]")
            console.print(Panel(
                responses["consensus"],
                title="Unified Consensus Response",
                border_style="green"
            ))
            
            console.print("\n[bold blue]Individual Contributions:[/bold blue]")
            for llm_name, response in responses["individual_responses"].items():
                console.print(Panel(
                    response,
                    title=f"{llm_name}'s Response",
                    border_style="blue"
                ))
        else:
            console.print("\n[bold yellow]‚ö†Ô∏è No Consensus Reached - Final Individual Responses:[/bold yellow]")
            for llm_name, response in responses.items():
                console.print(Panel(
                    response,
                    title=f"{llm_name} Final Response",
                    border_style="yellow"
                ))
            
    except Exception as e:
        console.print(f"\n[red]Error: {str(e)}[/red]")
        if debug:
            import traceback
            console.print(traceback.format_exc())
    finally:
        db_session.close()

@cli.command()
@click.argument('discussion_id', type=int)
@click.option('--debug', is_flag=True, help='Enable debug logging')
def view_discussion(discussion_id, debug):
    """View a previous discussion by ID."""
    if debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    db_session = get_db_session()
    engine = ConsensusEngine([], db_session)
    
    try:
        discussion = asyncio.run(engine.load_discussion(discussion_id))
        if not discussion:
            console.print(f"[red]No discussion found with ID {discussion_id}[/red]")
            return
            
        console.print(Panel(
            discussion['prompt'],
            title="Original Prompt",
            border_style="blue"
        ))
        
        if discussion['consensus_reached']:
            console.print("\n[bold green]‚úÖ Consensus Reached[/bold green]")
            console.print(Panel(
                discussion['final_consensus'],
                title="Final Consensus",
                border_style="green"
            ))
        else:
            console.print("\n[bold yellow]‚ùå No Consensus Reached[/bold yellow]")

        for round in discussion['rounds']:
            console.print(f"\n[bold blue]Round {round['round_number']}:[/bold blue]")
            for response in round['responses']:
                console.print(Panel(
                    response['response'],
                    title=response['llm_name'],
                    border_style="green"
                ))
    except Exception as e:
        console.print(f"[red]Error: {str(e)}[/red]")
        if debug:
            import traceback
            console.print(traceback.format_exc())
    finally:
        db_session.close()

@cli.command()
@click.option('--port', default=7860, help='Port to run the web interface on')
@click.option('--host', default="127.0.0.1", help='Host to run the web interface on')
@click.option('--debug', is_flag=True, help='Enable debug mode')
def web(port, host, debug):
    """Launch the Gradio web interface."""
    if debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    from .web import GradioInterface
    app = GradioInterface()
    app.launch(host=host, port=port, debug=debug)

if __name__ == "__main__":
    cli()
#src/consensus_engine/init.py (binary file - contents excluded)
